{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AndrewstheBuilder/machine_translation_French-Wolof/blob/main/RNN_LSTM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dEWZ1wO0dPgY"
      },
      "source": [
        "Definition of RNN from Karpathy:\n",
        "RNN Forward Pass Template(This is the paradigm for most of them probably. I have already seen two implementations following this paradigm):\n",
        "```python\n",
        "class RNN:\n",
        "  # ...\n",
        "  def step(self, x):\n",
        "    # update the hidden state\n",
        "    self.h = np.tanh(np.dot(self.W_hh, self.h) + np.dot(self.W_xh, x)) # self.h is the hidden state, x is the input.\n",
        "    # those weight matrices W_hh and W_xh are solely used for the hidden state and input respectively\n",
        "    # compute the output vector\n",
        "    y = np.dot(self.W_hy, self.h) # self.W_hy is solely used for the hidden state\n",
        "    return y\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZpfsRPOHdPga"
      },
      "source": [
        "Why is this addition used with the hidden state and the current input. How can I think about it conceptually? I think it makes sense on a abstract level. The previous sequence is influencing the current input. Also the third *np.dot(self.W_hy, self.h)*. Why do we do that third step?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bv0gmB71dPga"
      },
      "outputs": [],
      "source": [
        "# RNN Implementation from Grokking Learning\n",
        "import sys\n",
        "\n",
        "f = open('grokking_rnn/reviews.txt')\n",
        "raw_reviews = f.readlines()\n",
        "f.close()\n",
        "\n",
        "f = open('grokking_rnn/labels.txt')\n",
        "raw_labels = f.readlines()\n",
        "f.close()\n",
        "\n",
        "tokens = list(map(lambda x:set(x.split(\" \")),raw_reviews))\n",
        "\n",
        "vocab = set()\n",
        "for sent in tokens:\n",
        "    for word in sent:\n",
        "        if(len(word)>0):\n",
        "            vocab.add(word)\n",
        "vocab = list(vocab)\n",
        "\n",
        "word2index = {}\n",
        "for i,word in enumerate(vocab):\n",
        "    word2index[word]=i\n",
        "\n",
        "input_dataset = list()\n",
        "for sent in tokens:\n",
        "    sent_indices = list()\n",
        "    for word in sent:\n",
        "        try:\n",
        "            sent_indices.append(word2index[word])\n",
        "        except:\n",
        "            \"\"\n",
        "    input_dataset.append(list(set(sent_indices))) # Meaning you do not include duplicate words in your input dataset.\n",
        "    # And the ordering of words do not matter in the input dataset.\n",
        "    # Just the content of words and they are only accounted for a single occurrence.\n",
        "target_dataset = list()\n",
        "for label in raw_labels:\n",
        "    if label == 'positive\\n':\n",
        "        target_dataset.append(1)\n",
        "    else:\n",
        "        target_dataset.append(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "px60IMT3dPgb"
      },
      "outputs": [],
      "source": [
        "raw_reviews[:1]\n",
        "# raw_labels[:2]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "naX4oFHrdPgc"
      },
      "outputs": [],
      "source": [
        "tokens[:1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cU-vpAcOdPgd"
      },
      "outputs": [],
      "source": [
        "input_dataset[:1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TH-_jf5PdPge"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NhF7-w2GdPgf"
      },
      "outputs": [],
      "source": [
        "import sys,random,math\n",
        "from collections import Counter\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "\n",
        "np.random.seed(1)\n",
        "random.seed(1)\n",
        "f = open('grokking_rnn/reviews.txt')\n",
        "raw_reviews = f.readlines()\n",
        "f.close()\n",
        "\n",
        "tokens = list(map(lambda x:(x.split(\" \")),raw_reviews))\n",
        "wordcnt = Counter()\n",
        "for sent in tokens:\n",
        "    for word in sent:\n",
        "        wordcnt[word] -= 1\n",
        "vocab = list(set(map(lambda x:x[0],wordcnt.most_common())))\n",
        "# It puts the least common words up in the beginning of the vocab\n",
        "# Why is that?\n",
        "\n",
        "word2index = {}\n",
        "for i,word in enumerate(vocab):\n",
        "    word2index[word]=i\n",
        "\n",
        "concatenated = list()\n",
        "input_dataset = list()\n",
        "for sent in tokens:\n",
        "    sent_indices = list()\n",
        "    for word in sent:\n",
        "        try:\n",
        "            sent_indices.append(word2index[word])\n",
        "            concatenated.append(word2index[word])\n",
        "        except:\n",
        "            \"\"\n",
        "    input_dataset.append(sent_indices)\n",
        "concatenated = np.array(concatenated) # contenated is used later for????\n",
        "# Its the same as input_dataset except that it does not get shuffled like below\n",
        "random.shuffle(input_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dd2j30VWdPgg"
      },
      "outputs": [],
      "source": [
        "alpha, iterations = (0.05, 2)\n",
        "hidden_size,window,negative = (50,2,5)\n",
        "\n",
        "weights_0_1 = (np.random.rand(len(vocab),hidden_size) - 0.5) * 0.2\n",
        "weights_1_2 = np.random.rand(len(vocab),hidden_size)*0\n",
        "\n",
        "layer_2_target = np.zeros(negative+1)\n",
        "layer_2_target[0] = 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ud0IPyigdPgh",
        "outputId": "cff0f3e1-b1c1-4805-a102-f5f8dba94da4"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Find a good way to display the distribution of weights\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Generate sample data\n",
        "data = np.random.rand(10, 10)\n",
        "# print(data.shape)\n",
        "# print(weights_0_1.shape)\n",
        "# print(weights_0_1[:10].shape)\n",
        "# print(data)\n",
        "# print(weights_0_1[:10])\n",
        "\n",
        "# Create heatmap using matplotlib\n",
        "plt.imshow(data, cmap='viridis', interpolation='nearest')\n",
        "plt.colorbar()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZpKR5KvRdPgj"
      },
      "source": [
        "### Write a neural network to predict whether the sentiment of the review is positive or negative"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "FS4skx_5dPgk"
      },
      "outputs": [],
      "source": [
        "# Preprocessing Step for IMBD data\n",
        "import sys\n",
        "\n",
        "f = open('reviews.txt')\n",
        "raw_reviews = f.readlines()\n",
        "f.close()\n",
        "\n",
        "f = open('labels.txt')\n",
        "raw_labels = f.readlines()\n",
        "f.close()\n",
        "\n",
        "tokens = list(map(lambda x:set(x.split(\" \")), raw_reviews))\n",
        "\n",
        "vocab = set()\n",
        "for sent in tokens:\n",
        "    for word in sent:\n",
        "        if(len(word)>0):\n",
        "            vocab.add(word)\n",
        "vocab = list(vocab)\n",
        "\n",
        "word2index = {}\n",
        "for i, word in enumerate(vocab):\n",
        "    word2index[word] = i\n",
        "\n",
        "input_dataset = list()\n",
        "for sent in tokens:\n",
        "    sent_indices = list()\n",
        "    for word in sent:\n",
        "        try:\n",
        "            sent_indices.append(word2index[word])\n",
        "        except:\n",
        "            \"\"\n",
        "    input_dataset.append(list(set(sent_indices)))\n",
        "\n",
        "target_dataset = list()\n",
        "for label in raw_labels:\n",
        "    if label == 'positive\\n':\n",
        "        target_dataset.append(1)\n",
        "    else:\n",
        "        target_dataset.append(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ztu3hve0dPgk",
        "outputId": "9163d421-8bc8-4f19-bf70-c42cb7a886c1"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-79-7d51902177a9>:11: RuntimeWarning: overflow encountered in exp\n",
            "  return 1/(1+np.exp(-x))\n"
          ]
        }
      ],
      "source": [
        "# Actual Neural Network\n",
        "# My implementation\n",
        "# Stopped at forward propagation because there is a exp overflow issue\n",
        "import numpy as np\n",
        "np.random.seed(1)\n",
        "\n",
        "# Define a activation function --> Relu and Relu2Deriv\n",
        "relu = lambda x: (x>0) * x\n",
        "relu2deriv = lambda x: x>0\n",
        "\n",
        "# Output activation function to get a value between 0 and 1\n",
        "def sigmoid(x):\n",
        "    return 1/(1+np.exp(-x))\n",
        "\n",
        "alpha, iterations = (0.01, 2)\n",
        "hidden_size = 100\n",
        "\n",
        "weights_0_1 = 0.2*np.random.random((len(vocab), hidden_size)) - 0.1\n",
        "weights_1_2 = 0.2*np.random.random((hidden_size, 1)) - 0.1\n",
        "# Notes on weights_1_2\n",
        "# the network will output one answer if its closer to a zero then its 'negative'\n",
        "# And if its closer to 1 then its positive. The threshold is 0.5 and up is 'positive'.\n",
        "\n",
        "# Could adding a bias get rid of the need for scaling the weights above??\n",
        "bias_0_1 = 0.2*np.random.random((hidden_size)) - 0.1\n",
        "bias_1_2 = 0.2*np.random.random((1,1)) - 0.1\n",
        "\n",
        "for j in range(iterations):\n",
        "    error, correct_cnt =(0.0, 0)\n",
        "\n",
        "    for i in range(len(input_dataset[:10])):\n",
        "        row  = input_dataset[i]\n",
        "        label_number = target_dataset[i]\n",
        "        padded_row = list()\n",
        "        if(len(row) < len(vocab)):\n",
        "            # pad input with -1s\n",
        "            padded_row += row + [-1] * (len(vocab) - len(row))\n",
        "        elif(len(row) == len(vocab)):\n",
        "            # the row does not need any padding\n",
        "            padded_row += row\n",
        "        else:\n",
        "            raise Exception('Length of row exceeded length of vocab which should not be possible.')\n",
        "        layer_0 = padded_row\n",
        "        layer_1 = np.dot(layer_0,weights_0_1) #+ bias_0_1\n",
        "#         print('before:',layer_1.shape)\n",
        "#         print('bias_0_1.shape',bias_0_1.shape)\n",
        "#         layer_1 += bias_0_1\n",
        "#         print('after:',layer_1.shape)\n",
        "        layer_1 = relu(layer_1)\n",
        "        layer_2 = np.dot(layer_1, weights_1_2) #+ bias_1_2\n",
        "        layer_2 = sigmoid(layer_2)\n",
        "        error += np.sum((layer_2-label_number)**2) # Mean Squared Error\n",
        "#         print('layer_2',layer_2)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "uploaded = files.upload()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 109
        },
        "id": "zb-5lvKZdakk",
        "outputId": "9c988628-ed56-4495-be58-a53ba6b8f167"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-47b60609-b2a6-4f2f-a8a9-396d24e086f4\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-47b60609-b2a6-4f2f-a8a9-396d24e086f4\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving labels.txt to labels.txt\n",
            "Saving reviews.txt to reviews.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rNUaJrLZwWH0",
        "outputId": "7d373309-d559-4d4d-d88b-cdf5b2279854"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a0h-Qis7dPgl",
        "outputId": "e135b8a5-6727-4c05-922d-310507bbbbf7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iter:0 Progress:95.99% Training Accuracy:0.8335%\n",
            "Iter:1 Progress:95.99% Training Accuracy:0.8667916666666666%\n",
            "Test accuracy:0.85\n"
          ]
        }
      ],
      "source": [
        "# Grokking Deep Learning Implementation\n",
        "import numpy as np\n",
        "np.random.seed(1)\n",
        "\n",
        "def sigmoid(x):\n",
        "    return 1/(1 + np.exp(-x))\n",
        "\n",
        "alpha, iterations = (0.01, 2)\n",
        "hidden_size = 100\n",
        "\n",
        "weights_0_1 = 0.2*np.random.random((len(vocab), hidden_size)) - 0.1 # This will contain the trained word embeddings!!!!\n",
        "weights_1_2 = 0.2*np.random.random((hidden_size,1)) - 0.1\n",
        "\n",
        "correct, total = (0,0)\n",
        "for iter in range(iterations):\n",
        "\n",
        "    for i in range(len(input_dataset)-1000): # trains on first 24,000 reviews\n",
        "\n",
        "        x,y = (input_dataset[i],target_dataset[i])\n",
        "        layer_1 = sigmoid(np.sum(weights_0_1[x], axis=0)) # embed + sigmoid. Why sigmoid here?...\n",
        "        # TODO: come back to sum and relearn the significance of the axis\n",
        "        layer_2 = sigmoid(np.dot(layer_1, weights_1_2))\n",
        "\n",
        "        layer_2_delta = layer_2 - y # raw error. Compares the prediction with the truth\n",
        "        layer_1_delta = layer_2_delta.dot(weights_1_2.T) # Backpropagation\n",
        "\n",
        "        weights_0_1[x] -= layer_1_delta * alpha\n",
        "        weights_1_2 -= np.outer(layer_1,layer_2_delta) * alpha\n",
        "\n",
        "        if(np.abs(layer_2_delta) < 0.5):\n",
        "            # How is it being less than 0.5 mean that the prediction is correct??\n",
        "            # Why is the threshold 0.5? Is it a heuristic?\n",
        "            correct += 1\n",
        "        total += 1\n",
        "        if(i % 10 == 9):\n",
        "            progress = str(i/float(len(input_dataset)))\n",
        "            sys.stdout.write('\\rIter:' + str(iter)\\\n",
        "                                + ' Progress:'+progress[2:4]\\\n",
        "                                + '.'+progress[4:6]\\\n",
        "                                +'% Training Accuracy:'\\\n",
        "                                +str(correct/float(total)) + '%')\n",
        "    print()\n",
        "# this is the test data set portion of the code\n",
        "correct, total = (0,0)\n",
        "for i in range(len(input_dataset) - 1000,len(input_dataset)):\n",
        "\n",
        "    x = input_dataset[i]\n",
        "    y = target_dataset[i]\n",
        "\n",
        "    layer_1 = sigmoid(np.sum(weights_0_1[x], axis=0))\n",
        "    layer_2 = sigmoid(np.dot(layer_1, weights_1_2))\n",
        "\n",
        "    if(np.abs(layer_2 - y) < 0.5):\n",
        "        correct += 1\n",
        "    total += 1\n",
        "print(\"Test accuracy:\" + str(correct/float(total)))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Filling in the blank"
      ],
      "metadata": {
        "id": "8Y7CZ_ean6-i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sys,random,math\n",
        "from collections import Counter\n",
        "import numpy as np\n",
        "\n",
        "np.random.seed(1)\n",
        "random.seed(1)\n",
        "f = open('/content/drive/MyDrive/ml-olympiad-machine-translation-french-wolof/reviews.txt')\n",
        "raw_reviews = f.readlines()\n",
        "f.close()\n",
        "\n",
        "tokens = list(map(lambda x:(x.split(\" \")),raw_reviews))\n",
        "wordcnt = Counter()\n",
        "for sent in tokens:\n",
        "    for word in sent:\n",
        "        wordcnt[word] -= 1\n",
        "vocab = list(set(map(lambda x:x[0],wordcnt.most_common()))) # this puts the least common words in first\n",
        "# And its a set of words so there are no duplicate words added to the set. Set python DataStructure(DS)\n",
        "\n",
        "word2index = {}\n",
        "for i,word in enumerate(vocab):\n",
        "    word2index[word]=i\n",
        "\n",
        "concatenated = list()\n",
        "input_dataset = list()\n",
        "for sent in tokens:\n",
        "    sent_indices = list()\n",
        "    for word in sent:\n",
        "        try:\n",
        "            sent_indices.append(word2index[word])\n",
        "            concatenated.append(word2index[word])\n",
        "        except:\n",
        "            \"\"\n",
        "    input_dataset.append(sent_indices)\n",
        "concatenated = np.array(concatenated)\n",
        "random.shuffle(input_dataset)"
      ],
      "metadata": {
        "id": "093YV6-UnzVM"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "alpha, iterations = (0.05, 2)\n",
        "hidden_size,window,negative = (50,2,5)\n",
        "\n",
        "weights_0_1 = (np.random.rand(len(vocab),hidden_size) - 0.5) * 0.2\n",
        "weights_1_2 = np.random.rand(len(vocab),hidden_size)*0 # The weights from layers 1 - 2 are all 0s.\n",
        "\n",
        "# What are these two for?\n",
        "layer_2_target = np.zeros(negative+1)\n",
        "layer_2_target[0] = 1\n",
        "\n",
        "# Find similarity between words using the embedding weight and euclidean distance formula\n",
        "def similar(target='beautiful'):\n",
        "  target_index = word2index[target]\n",
        "\n",
        "  scores = Counter()\n",
        "  for word,index in word2index.items():\n",
        "    raw_difference = weights_0_1[index] - (weights_0_1[target_index])\n",
        "    squared_difference = raw_difference * raw_difference\n",
        "    scores[word] = -math.sqrt(sum(squared_difference))\n",
        "  return scores.most_common(10)\n",
        "\n",
        "def sigmoid(x):\n",
        "    return 1/(1 + np.exp(-x))\n",
        "\n",
        "for rev_i,review in enumerate(input_dataset * iterations):\n",
        "  for target_i in range(len(review)):\n",
        "    # since it's really expensive to predict every vocabulary\n",
        "    # we're only going to predict a random subset\n",
        "    # TODO understand what this target_samples is...\n",
        "    target_samples = [review[target_i]]+list(concatenated\\\n",
        "    [(np.random.rand(negative)*len(concatenated)).astype('int').tolist()])\n",
        "\n",
        "    # TODO: understand how we use left and right context\n",
        "    # Left context is a window of words before the target\n",
        "    # Right context is a window of words after the target.\n",
        "    # The max and min functions are used to avoid out of bounds exceptions\n",
        "    # left and right contexts are lists\n",
        "    left_context = review[max(0,target_i-window):target_i]\n",
        "    right_context = review[target_i+1:min(len(review),target_i+window)]\n",
        "\n",
        "    # Why do we take the mean of weights_0_1 after getting the embeddings for the left context and\n",
        "    # right context?\n",
        "    # I think this can be explained by the word2vec negative sampling paper\n",
        "    layer_1 = np.mean(weights_0_1[left_context+right_context],axis=0)\n",
        "    layer_2 = sigmoid(layer_1.dot(weights_1_2[target_samples].T))\n",
        "    # Why do we take the target_samples and put it in weights_1_2??\n",
        "    # I think I do not understand weights_1_2\n",
        "    layer_2_delta = layer_2 - layer_2_target\n",
        "    layer_1_delta = layer_2_delta.dot(weights_1_2[target_samples])\n",
        "\n",
        "    weights_0_1[left_context+right_context] -= layer_1_delta * alpha\n",
        "    weights_1_2[target_samples] -= np.outer(layer_2_delta,layer_1)*alpha\n",
        "\n",
        "  if(rev_i % 250 == 0):\n",
        "    sys.stdout.write('\\rProgress:'+str(rev_i/float(len(input_dataset)*iterations)) + \"   \" + str(similar('terrible')))\n",
        "  # sys.stdout.write('\\rProgress:'+str(rev_i/float(len(input_dataset)*iterations)))\n",
        "print(similar('terrible'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 379
        },
        "id": "iG_A70lHpAxd",
        "outputId": "66cfd5ca-442f-423c-b8fc-88b374badf74"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Progress:0.065   [('terrible', -0.0), ('horrible', -1.3072847966446794), ('pathetic', -1.631275286145734), ('hilarious', -1.647451465875297), ('convincing', -1.6534201493797647), ('controversial', -1.7861199240472359), ('disaster', -1.7968731919575058), ('awesome', -1.8089449636271586), ('w', -1.8130254117424283), ('guilty', -1.8202830815864564)]"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-047a949e19e1>\u001b[0m in \u001b[0;36m<cell line: 25>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0mweights_0_1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mleft_context\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mright_context\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0mlayer_1_delta\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m     \u001b[0mweights_1_2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtarget_samples\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mouter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer_2_delta\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlayer_1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m   \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrev_i\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m250\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/numpy/core/numeric.py\u001b[0m in \u001b[0;36m_outer_dispatcher\u001b[0;34m(a, b, out)\u001b[0m\n\u001b[1;32m    835\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    836\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 837\u001b[0;31m \u001b[0;32mdef\u001b[0m \u001b[0m_outer_dispatcher\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    838\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    839\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(similar('movie'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3wo1k4214JaZ",
        "outputId": "f6e1b839-3219-4584-b70f-16144c3ea62d"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('movie', -0.0), ('film', -3.8947651212371484), ('documentary', -4.7505790862901005), ('program', -4.805620089328229), ('project', -5.230649447411363), ('show', -5.237190652793311), ('game', -5.311411939473912), ('programme', -5.3844935178311815), ('miniseries', -5.396019546443762), ('contestant', -5.418593183157244)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('/content/drive/MyDrive/ml-olympiad-machine-translation-french-wolof/weights/grokking_rnn_weights_0_1.npy')\n",
        "np.save('/content/drive/MyDrive/ml-olympiad-machine-translation-french-wolof/weights/grokking_rnn_weights_0_1.npy', weights_0_1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xCYq6-KyaZsE",
        "outputId": "9e2e7ace-6c03-45a9-dd7a-aff446751ca0"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/ml-olympiad-machine-translation-french-wolof/weights/grokking_rnn_weights_0_1.npy\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np"
      ],
      "metadata": {
        "id": "X54hXrukMStJ"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "weights_0_1 = np.load('/content/drive/MyDrive/ml-olympiad-machine-translation-french-wolof/weights/grokking_rnn_weights_0_1.npy')\n",
        "print(weights_0_1.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Pl33G31MGT2",
        "outputId": "31bcaac0-ea7c-4ba5-c8dd-7328856690a8"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(74075, 50)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Average Sentence Embeddings\n",
        "# A Method to compare sentences of possibly differing lengths\n",
        "import numpy as np\n",
        "# Calculate the normed_weights of weights_0_1(the embedding weight)\n",
        "# Q1. How are we using the norm to calculate the average?\n",
        "# Q2. What happens to this average as the number of words in the sentence increases?\n",
        "norms = np.sum(weights_0_1 * weights_0_1,axis=1, keepdims=True) # What is a norm in math? Intuitively?\n",
        "# print('weights_0_1.shape:',weights_0_1.shape)\n",
        "# print('norms.shape',norms.shape)\n",
        "normed_weights = weights_0_1 * norms\n",
        "\n",
        "def make_sent_vect(words):\n",
        "  # Make sure words are in word2index before converting to indices\n",
        "  indices = list(map(lambda x:word2index[x], \\\n",
        "                     filter(lambda x:x in word2index, words)))\n",
        "  # Return the mean of the of normed weights == average?\n",
        "  return np.mean(normed_weights[indices], axis=0)\n",
        "\n",
        "# Append vectorized review into reviews2vectors\n",
        "reviews2vectors = list()\n",
        "for review in tokens:\n",
        "  reviews2vectors.append(make_sent_vect(review))\n",
        "reviews2vectors = np.array(reviews2vectors)\n",
        "\n",
        "def most_similar_reviews(review):\n",
        "  v = make_sent_vect(review)\n",
        "  scores = Counter()\n",
        "  # do a dot product of a single review(averaged word embeddings)\n",
        "  # with all of the reviews(which are in vector form)\n",
        "  # calculate scores using reviews2vectors dot product with value\n",
        "  # What is up with this dot product why does it work?\n",
        "  for i,val in enumerate(reviews2vectors.dot(v)):\n",
        "    scores[i] = val\n",
        "  most_similar = list()\n",
        "\n",
        "\n",
        "  for idx,score in scores.most_common(5):\n",
        "    most_similar.append(raw_reviews[idx][0:100])\n",
        "  return most_similar\n",
        "most_similar_reviews(['lust', 'sex'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "47XLUYnx5VOF",
        "outputId": "bf8b4c45-dbe5-4859-f455-3636fd99ddba"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['this is my favorite show . i think it is utterly brilliant . thanks to david chase for bringing this',\n",
              " 'i  ve read one comment which labeled this film  trash  and  a waste  br    br   of time .  i think t',\n",
              " 'this is not a love song .  br    br   my one word summary of this film would be excellent  .  br    ',\n",
              " 'i will never be a member of any club that would have me   br    br   especially this one .  br    br',\n",
              " '  br    br   human body    wow .  br    br   there are about       sunrises in human life . . . .  b']"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Chapter 12 Grokking Deep Learning\n",
        "## Toy Example to see how the idea of Transition Matrices will work\n",
        "import numpy as np\n",
        "\n",
        "def softmax(x_):\n",
        "  x = np.atleast_2d(x_)\n",
        "  temp = np.exp(x)\n",
        "  return temp/np.sum(temp, axis=1, keepdims=True)\n",
        "\n",
        "# word embeddings\n",
        "word_vects = {}\n",
        "word_vects['yankees'] = np.array([[0., 0., 0.,]])\n",
        "word_vects['bears'] = np.array([[0., 0., 0.,]])\n",
        "word_vects['braves'] = np.array([[0., 0., 0.,]])\n",
        "word_vects['red'] = np.array([[0., 0., 0.,]])\n",
        "word_vects['sox'] = np.array([[0., 0., 0.,]])\n",
        "word_vects['lose'] = np.array([[0., 0., 0.,]])\n",
        "word_vects['defeat'] = np.array([[0., 0., 0.,]])\n",
        "word_vects['beat'] = np.array([[0., 0., 0.,]])\n",
        "word_vects['tie'] = np.array([[0., 0., 0.,]])\n",
        "\n",
        "# Sentence embedding to output classification weights??? Two in one weights??\n",
        "sent2output = np.random.rand(3, len(word_vects))\n",
        "\n",
        "identity = np.eye(3) # Transition matrix weights\n",
        "\n",
        "# Forward propagation\n",
        "# sent2output is a weight matrix to predict the next word given a sentence vector\n",
        "# of length 3.\n",
        "# Below is example for Forward Propagation of sentence \"red sox defeat\" -> \"yankees\"\n",
        "layer_0 = word_vects[\"red\"]\n",
        "# The adding of the word_vects of 'sox' and 'defeat' are whats new in this chapter\n",
        "# The rest of it is a structure we have seen before\n",
        "layer_1 = layer_0.dot(identity) + word_vects['sox']\n",
        "layer_2 = layer_1.dot(identity) + word_vects['defeat']\n",
        "\n",
        "pred = softmax(layer_2.dot(sent2output))\n",
        "print(pred)\n",
        "\n",
        "# Back Propagation\n",
        "\n",
        "# y is the one hot vector for 'yankees'\n",
        "y = np.array([1,0,0,0,0,0,0,0,0])\n",
        "\n",
        "# I do not feel confident I could come up with these backprop\n",
        "# statements\n",
        "pred_delta = pred - y\n",
        "layer_2_delta = pred_delta.dot(sent2output.T)\n",
        "defeat_delta = layer_2_delta * 1\n",
        "layer_1_delta = layer_2_delta.dot(identity.T)\n",
        "sox_delta = layer_1_delta * 1\n",
        "layer_0_delta = layer_1_delta.dot(identity.T)\n",
        "alpha = 0.01\n",
        "word_vects['red'] -= layer_0_delta * alpha\n",
        "word_vects['sox'] -= sox_delta * alpha\n",
        "word_vects['defeat'] -= defeat_delta * alpha\n",
        "# Why do we use np.outer here for back prop?\n",
        "identity -= np.outer(layer_0,layer_1_delta) * alpha\n",
        "identity -= np.outer(layer_1,layer_2_delta) * alpha\n",
        "sent2output -= np.outer(layer_2,pred_delta) * alpha"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w8pyWQDBJ7PQ",
        "outputId": "a41af0e7-e575-4564-9db8-c4d62c5c4a81"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.11111111 0.11111111 0.11111111 0.11111111 0.11111111 0.11111111\n",
            "  0.11111111 0.11111111 0.11111111]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: stopped at page 222 write out code for rest of RNN chapter 12"
      ],
      "metadata": {
        "id": "NCv5yFunUAFN"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}